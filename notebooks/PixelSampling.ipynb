{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the fundamental issues with expressing planetary and stellar surface intensities or albedos as a spherical harmonic expansion is that it can be difficult to enforce positivity of the surface map everywhere. In particular, there's no way to exactly express the positivity constraint as a prior probability distribution on the spherical harmonic coefficientss. This is a problem not only because the posterior will have support for unphysical solutions, but (more importantly) because a non-negativity prior can be very constraining, greatly reducing the number of degeneracies in the problem.\n",
    "\n",
    "The approach we recommend here is to **perform the sampling in pixel space, but evaluate the model in spherical harmonic space.** This combines the best of both worlds: pixel space makes it easy to impose physical constraints on the intensity, while in spherical harmonic space the flux evaluation is analytic, fast, and differentiable. Plus, it's easy to construct linear operators that take us back and forth between these two spaces (at some finite resolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "%run notebook_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import starry\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import pymc3 as pm\n",
    "import exoplanet\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "\n",
    "starry.config.lazy = True\n",
    "starry.config.quiet = True\n",
    "cmap = plt.get_cmap(\"plasma\")\n",
    "cmap.set_under(\"#666666\")\n",
    "cmap.set_over(\"w\")\n",
    "cnorm = lambda: colors.Normalize(vmin=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first generate a mock dataset so we can experiment with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock surface map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a stellar (or planetary) surface map with several large, distinct spots. We'll do this at high ($l=20$) resolution so we can get crisp features without significant ringing. Note that we set the inclination to $85^\\circ$ so we can break the north-south degeneracy from map viewed edge-on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "ydeg_tru = 20\n",
    "inc = 85\n",
    "\n",
    "# Instantiate\n",
    "map_tru = starry.Map(ydeg_tru, inc=inc)\n",
    "map_tru.add_spot(amp=-0.03, relative=False, sigma=0.05, lat=30, lon=0)\n",
    "map_tru.add_spot(amp=-0.06, relative=False, sigma=0.1, lat=-20, lon=60)\n",
    "map_tru.add_spot(amp=-0.03, relative=False, sigma=0.05, lat=10, lon=150)\n",
    "map_tru.add_spot(amp=-0.03, relative=False, sigma=0.05, lat=60, lon=-90)\n",
    "map_tru.add_spot(amp=-0.025, relative=False, sigma=0.04, lat=-30, lon=-90)\n",
    "map_tru.add_spot(amp=-0.025, relative=False, sigma=0.04, lat=0, lon=-150)\n",
    "map_tru.amp = 1.0\n",
    "y_tru = np.array(map_tru.y.eval())\n",
    "amp_tru = 1.0\n",
    "map_tru.show(projection=\"moll\", colorbar=True, norm=cnorm(), cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spot amplitudes were tuned so the intensity remains non-negative everywhere. The background intensity (about 0.4) doesn't matter for our purposes, since we're going to normalize the flux when doing inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock light curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a mock light curve. As our object rotates, we'll transit it with several occultors of different sizes and at different impact parameters. This isn't necessarily realistic, but the point here is to ensure our light curve encodes information about the map at a variety of positions and a variety of scales.\n",
    "\n",
    "We begin by constructing the flux design matrix, which dots into the spherical harmonic coefficient vector to give us the flux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "npts = 1000\n",
    "prot = 1.0 / 7.0\n",
    "bo = [-0.5, 0.25, 0.75, 0.5]\n",
    "ro = [0.5, 0.75, 0.3, 0.2]\n",
    "time = np.linspace(0, 1, npts)\n",
    "theta = (360.0 * time / prot).reshape(len(ro), -1)\n",
    "X_tru = np.vstack(\n",
    "    [\n",
    "        map_tru.design_matrix(\n",
    "            theta=theta[i],\n",
    "            xo=np.linspace(-1 - ro[i], 1 + ro[i], len(theta[i])),\n",
    "            yo=bo[i],\n",
    "            ro=ro[i],\n",
    "        ).eval()\n",
    "        for i in range(4)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the light curve, add a tiny bit of noise, and median-normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the light curve\n",
    "flux_tru = amp_tru * X_tru.dot(y_tru)\n",
    "\n",
    "# Add noise\n",
    "np.random.seed(0)\n",
    "ferr_tru = 1e-2\n",
    "flux = flux_tru + ferr_tru * np.random.randn(len(flux_tru))\n",
    "\n",
    "# Normalize\n",
    "norm = np.nanmedian(flux)\n",
    "flux = flux / norm\n",
    "ferr = ferr_tru / norm\n",
    "\n",
    "# Plot\n",
    "plt.plot(time, flux, \"k.\", alpha=0.3, label=\"observed\")\n",
    "plt.plot(time, flux_tru / norm, \"C0\", label=\"true\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"flux\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to do inference. For simplicity, we'll assume we know all the information about the occultors and the object's rotation rate **exactly**, so the only unknown is the surface map.\n",
    "\n",
    "We'll begin by pre-computing some of the linear operators we'll use throughout this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "ydeg_inf = 15\n",
    "\n",
    "# Instantiate\n",
    "map_inf = starry.Map(ydeg_inf, inc=inc)\n",
    "X_inf = X_tru[:, : (ydeg_inf + 1) ** 2]\n",
    "lat, lon, Y2P, P2Y, Dx, Dy = map_inf.get_pixel_transforms(oversample=2)\n",
    "npix = lat.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll discuss these in more detail below, but `X_inf` is our design matrix at the resolution we're doing inference at, `Y2P` and `P2Y` are the linear operators that transform back and forth between spherical harmonic coefficients and pixel intensities, and `Dx` and `Dy` are the numerical derivative operators, which dot into a vector of pixel intensities to produce the vector of $x$- and $y$-derivatives at those pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll consider various inference methods below. Let's store all our solutions in these variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmeth = 7\n",
    "labels = [\"L2\", \"U\", \"Beta\", \"U+Mix\", \"U+TV\", \"Aizawa\", \"U+TV+Mix\"]\n",
    "y = np.zeros((nmeth, (ydeg_inf + 1) ** 2))\n",
    "amp = np.zeros(nmeth)\n",
    "flux_model = np.zeros((nmeth, npts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot(y, amp, flux_model):\n",
    "    \"\"\"\n",
    "    Compare the inferred map to the true map.\n",
    "    \n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    fig.subplots_adjust(wspace=0.15)\n",
    "    ax = [\n",
    "        plt.subplot2grid((3, 2), (0, 0), colspan=1, rowspan=1),\n",
    "        plt.subplot2grid((3, 2), (0, 1), colspan=1, rowspan=1),\n",
    "        plt.subplot2grid((3, 2), (1, 0), colspan=2, rowspan=1),\n",
    "        plt.subplot2grid((3, 2), (2, 0), colspan=2, rowspan=1),\n",
    "    ]\n",
    "\n",
    "    # Show the true map\n",
    "    map_tru.show(\n",
    "        ax=ax[0], projection=\"moll\", colorbar=True, norm=cnorm(), cmap=cmap,\n",
    "    )\n",
    "\n",
    "    # Show the inferred map\n",
    "    map_inf.amp = amp\n",
    "    map_inf[1:, :] = y[1:]\n",
    "    map_inf.show(\n",
    "        ax=ax[1], projection=\"moll\", colorbar=True, norm=cnorm(), cmap=cmap,\n",
    "    )\n",
    "\n",
    "    # Show the flux model\n",
    "    ax[2].plot(time, flux, \"k.\", alpha=0.3, label=\"observed\")\n",
    "    ax[2].plot(time, flux_model, \"C1\", label=\"model\")\n",
    "    ax[2].legend(fontsize=10, loc=\"lower right\")\n",
    "    ax[2].set_xlabel(\"time\")\n",
    "    ax[2].set_ylabel(\"flux\")\n",
    "\n",
    "    # Show the pixel distributions\n",
    "    pix_tru = map_tru.render(projection=\"moll\").eval().flatten()\n",
    "    pix_tru /= np.nanmax(pix_tru)\n",
    "    pix_tmp = map_inf.render(projection=\"moll\").eval().flatten()\n",
    "    pix_tmp /= np.nanmax(pix_tmp)\n",
    "    ax[3].hist(pix_tru, bins=50, alpha=0.5, label=\"true\")\n",
    "    ax[3].hist(pix_tmp, bins=50, alpha=0.5, label=\"inferred\")\n",
    "    ax[3].legend()\n",
    "    ax[3].set_yticks([])\n",
    "    ax[3].set_ylabel(\"pixels\")\n",
    "    ax[3].set_xlabel(\"intensity\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first method is the usual one: L2 regularization on the spherical harmonic coefficients. The posterior is analytic and so we don't need to do any sampling. The only thing we need to decide on is the prior on the coefficients, which for simplicity we'll assume is a zero-mean Gaussian with covariance that's a scalar multiple of the identity. We just need to choose that scalar, which we'll call $\\lambda$. If we make $\\lambda$ small enough, we  actually **can** ensure the map is positive everywhere, since we're severely restricting the variability of the features about the mean (which is non-zero). The problem is that usually that ends up being too restrictive a prior, since we're forcing the pixel intensities to be clustered about the mean. That makes it difficult to capture surface maps like the one in our example, where the intensities are either high or low, but not often in between. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_inf.set_data(flux, C=ferr ** 2)\n",
    "map_inf.set_prior(L=1e-4)\n",
    "map_inf.solve(design_matrix=X_inf)\n",
    "y[0] = np.array(map_inf.y.eval())\n",
    "amp[0] = map_inf.amp.eval()\n",
    "flux_model[0] = amp[0] * X_inf.dot(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "plot(y[0], amp[0], flux_model[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel inference w/ uniform prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's instead sample in pixel space. We place a uniform prior on the pixels, then transform to spherical harmonics via the `P2Y` matrix, and use those to compute the flux. Note that the posterior we care about is *still* over the spherical harmonic coefficients, since that is the space in which we are computing the light curve model. The pixel sampling is merely a trick so we can enforce our prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(theano_config=dict(compute_test_value=\"ignore\")) as model:\n",
    "\n",
    "    # Uniform prior on the *pixels*\n",
    "    p = pm.Uniform(\"p\", lower=0.0, upper=1.0, shape=(npix,))\n",
    "    norm = pm.Normal(\"norm\", mu=0.5, sd=0.25)\n",
    "    x = norm * tt.dot(P2Y, p)\n",
    "\n",
    "    # Compute the flux\n",
    "    lc_model = tt.dot(X_inf, x)\n",
    "    pm.Deterministic(\"lc_model\", lc_model)\n",
    "    lc_model_guess = exoplanet.eval_in_model(lc_model)\n",
    "\n",
    "    # Store the Ylm coeffs. Note that `x` is the\n",
    "    # *amplitude-weighted* vector of spherical harmonic\n",
    "    # coefficients.\n",
    "    pm.Deterministic(\"amp\", x[0])\n",
    "    pm.Deterministic(\"y\", x / x[0])\n",
    "\n",
    "    # The likelihood function assuming known Gaussian uncertainty\n",
    "    pm.Normal(\"obs\", mu=lc_model, sd=ferr, observed=flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    soln = exoplanet.optimize(options=dict(maxiter=9999))\n",
    "    y[1] = np.array(soln[\"y\"])\n",
    "    amp[1] = soln[\"amp\"]\n",
    "    flux_model[1] = soln[\"lc_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "plot(y[1], amp[1], flux_model[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel inference w/ Beta prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the histogram of true pixel intensities above, it looks like we may be able to capture this behavior with a $Beta(0.5, 0.5)$ prior, which places most of it weight at 0 and 1. Let's try that and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(theano_config=dict(compute_test_value=\"ignore\")) as model:\n",
    "\n",
    "    # Beta prior on the *pixels*\n",
    "    p = pm.Beta(\"p\", alpha=0.5, beta=0.5, shape=(npix,))\n",
    "    norm = pm.Normal(\"norm\", mu=0.5, sd=0.25)\n",
    "    x = norm * tt.dot(P2Y, p)\n",
    "\n",
    "    # Compute the flux\n",
    "    lc_model = tt.dot(X_inf, x)\n",
    "    pm.Deterministic(\"lc_model\", lc_model)\n",
    "    lc_model_guess = exoplanet.eval_in_model(lc_model)\n",
    "\n",
    "    # Store the Ylm coeffs\n",
    "    pm.Deterministic(\"amp\", x[0])\n",
    "    pm.Deterministic(\"y\", x / x[0])\n",
    "\n",
    "    # The likelihood function assuming known Gaussian uncertainty\n",
    "    pm.Normal(\"obs\", mu=lc_model, sd=ferr, observed=flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    soln = exoplanet.optimize(options=dict(maxiter=9999))\n",
    "    y[2] = np.array(soln[\"y\"])\n",
    "    amp[2] = soln[\"amp\"]\n",
    "    flux_model[2] = soln[\"lc_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "plot(y[2], amp[2], flux_model[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel inference w/ Gaussian mixture prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also try a Gaussian mixture model to place weight at the two extrema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(theano_config=dict(compute_test_value=\"ignore\")) as model:\n",
    "\n",
    "    # Uniform prior on the *pixels*\n",
    "    p = pm.Uniform(\n",
    "        \"p\", lower=0.0, upper=1.0, shape=(npix,), testval=0.99 * np.ones(npix)\n",
    "    )\n",
    "    norm = pm.Normal(\"norm\", mu=0.5, sd=0.25)\n",
    "    x = norm * tt.dot(P2Y, p)\n",
    "\n",
    "    # Mixture of Gaussians\n",
    "    sig = 0.35\n",
    "    pm.Potential(\"mix\", -0.5 * tt.sum(tt.minimum((1 - p) ** 2, (p ** 2))) / sig ** 2)\n",
    "\n",
    "    # Compute the flux\n",
    "    lc_model = tt.dot(X_inf, x)\n",
    "    pm.Deterministic(\"lc_model\", lc_model)\n",
    "    lc_model_guess = exoplanet.eval_in_model(lc_model)\n",
    "\n",
    "    # Store the Ylm coeffs\n",
    "    pm.Deterministic(\"amp\", x[0])\n",
    "    pm.Deterministic(\"y\", x / x[0])\n",
    "\n",
    "    # The likelihood function assuming known Gaussian uncertainty\n",
    "    pm.Normal(\"obs\", mu=lc_model, sd=ferr, observed=flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    soln = exoplanet.optimize(options=dict(maxiter=9999))\n",
    "    y[3] = np.array(soln[\"y\"])\n",
    "    amp[3] = soln[\"amp\"]\n",
    "    flux_model[3] = soln[\"lc_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "plot(y[3], amp[3], flux_model[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel inference w/ TV prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place a total variation (TV) L1 prior on the magnitude of the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(theano_config=dict(compute_test_value=\"ignore\")) as model:\n",
    "\n",
    "    # Uniform prior on the *pixels*\n",
    "    p = pm.Uniform(\"p\", lower=0.0, upper=1.0, shape=(npix,))\n",
    "    norm = pm.Normal(\"norm\", mu=0.5, sd=0.25)\n",
    "    x = norm * tt.dot(P2Y, p)\n",
    "\n",
    "    # Apply the TV penalty\n",
    "    theta = 3.0\n",
    "    TV = tt.sum(tt.sqrt(tt.dot(Dx, p) ** 2 + tt.dot(Dy, p) ** 2))\n",
    "    pm.Potential(\"TV\", -TV / theta)\n",
    "\n",
    "    # Compute the flux\n",
    "    lc_model = tt.dot(X_inf, x)\n",
    "    pm.Deterministic(\"lc_model\", lc_model)\n",
    "    lc_model_guess = exoplanet.eval_in_model(lc_model)\n",
    "\n",
    "    # Store the Ylm coeffs\n",
    "    pm.Deterministic(\"amp\", x[0])\n",
    "    pm.Deterministic(\"y\", x / x[0])\n",
    "\n",
    "    # The likelihood function assuming known Gaussian uncertainty\n",
    "    pm.Normal(\"obs\", mu=lc_model, sd=ferr, observed=flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    soln = exoplanet.optimize(options=dict(maxiter=9999))\n",
    "    y[4] = np.array(soln[\"y\"])\n",
    "    amp[4] = soln[\"amp\"]\n",
    "    flux_model[4] = soln[\"lc_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "plot(y[4], amp[4], flux_model[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel inference w/ [Aizawa+20](https://arxiv.org/pdf/2004.03941.pdf) prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(theano_config=dict(compute_test_value=\"ignore\")) as model:\n",
    "\n",
    "    # Uniform prior on the *pixels*\n",
    "    p = pm.Uniform(\"p\", lower=0.0, upper=1.0, shape=(npix,))\n",
    "    norm = pm.Normal(\"norm\", mu=0.5, sd=0.25)\n",
    "    x = norm * tt.dot(P2Y, p)\n",
    "\n",
    "    # Apply the L1 penalty on the\n",
    "    # *difference between the pixels and unity*\n",
    "    # since we want sparseness relative to the\n",
    "    # background flux (which is unity)\n",
    "    lam = 1.0\n",
    "    pm.Potential(\"L1\", -lam * tt.sum(tt.abs_((1 - p))))\n",
    "\n",
    "    # Apply the TSV penalty as an L2 norm on the gradient\n",
    "    sig = 0.5\n",
    "    pm.Potential(\n",
    "        \"TSV\", -0.5 * tt.sum((tt.dot(Dx, p)) ** 2 + (tt.dot(Dy, p)) ** 2) / sig ** 2\n",
    "    )\n",
    "\n",
    "    # Compute the flux\n",
    "    lc_model = tt.dot(X_inf, x)\n",
    "    pm.Deterministic(\"lc_model\", lc_model)\n",
    "    lc_model_guess = exoplanet.eval_in_model(lc_model)\n",
    "\n",
    "    # Store the Ylm coeffs\n",
    "    pm.Deterministic(\"amp\", x[0])\n",
    "    pm.Deterministic(\"y\", x / x[0])\n",
    "\n",
    "    # The likelihood function assuming known Gaussian uncertainty\n",
    "    pm.Normal(\"obs\", mu=lc_model, sd=ferr, observed=flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    soln = exoplanet.optimize(options=dict(maxiter=9999))\n",
    "    y[5] = np.array(soln[\"y\"])\n",
    "    amp[5] = soln[\"amp\"]\n",
    "    flux_model[5] = soln[\"lc_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "plot(y[5], amp[5], flux_model[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel inference w/ TV + Gaussian mixture prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(theano_config=dict(compute_test_value=\"ignore\")) as model:\n",
    "\n",
    "    # Uniform prior on the *pixels*\n",
    "    # Initialize at the solution to the mixture model run\n",
    "    guess = amp[3] * np.maximum(0, np.dot(Y2P, y[3]))\n",
    "    scale = np.max(guess)\n",
    "    pad = 1e-2\n",
    "    guess = pad + guess / scale * (1 - 2 * pad)\n",
    "    p = pm.Uniform(\"p\", lower=0.0, upper=1.0, shape=(npix,), testval=guess)\n",
    "    norm = pm.Normal(\"norm\", mu=0.5, sd=0.25, testval=scale)\n",
    "    x = norm * tt.dot(P2Y, p)\n",
    "\n",
    "    # Mixture of Gaussians\n",
    "    sig = 0.35\n",
    "    pm.Potential(\"mix\", -0.5 * tt.sum(tt.minimum((1 - p) ** 2, (p ** 2))) / sig ** 2)\n",
    "\n",
    "    # Apply the TV penalty\n",
    "    theta = 3.0\n",
    "    TV = tt.sum(tt.sqrt(tt.dot(Dx, p) ** 2 + tt.dot(Dy, p) ** 2))\n",
    "    pm.Potential(\"TV\", -TV / theta)\n",
    "\n",
    "    # Compute the flux\n",
    "    lc_model = tt.dot(X_inf, x)\n",
    "    pm.Deterministic(\"lc_model\", lc_model)\n",
    "    lc_model_guess = exoplanet.eval_in_model(lc_model)\n",
    "\n",
    "    # Store the Ylm coeffs\n",
    "    pm.Deterministic(\"amp\", x[0])\n",
    "    pm.Deterministic(\"y\", x / x[0])\n",
    "\n",
    "    # The likelihood function assuming known Gaussian uncertainty\n",
    "    pm.Normal(\"obs\", mu=lc_model, sd=ferr, observed=flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    soln = exoplanet.optimize(options=dict(maxiter=9999))\n",
    "    y[6] = np.array(soln[\"y\"])\n",
    "    amp[6] = soln[\"amp\"]\n",
    "    flux_model[6] = soln[\"lc_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "plot(np.array(soln[\"y\"]), soln[\"amp\"], flux_model[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare all the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize=(15, 8))\n",
    "ax = ax.flatten()\n",
    "for axis in ax:\n",
    "    axis.axis(\"off\")\n",
    "\n",
    "# True map\n",
    "map_tru.show(\n",
    "    ax=ax[0], projection=\"moll\", colorbar=True, norm=cnorm(), cmap=cmap,\n",
    ")\n",
    "ax[0].annotate(\n",
    "    \"true\",\n",
    "    xy=(0, np.sqrt(2)),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(0, 5),\n",
    "    textcoords=\"offset points\",\n",
    "    ha=\"center\",\n",
    "    va=\"bottom\",\n",
    "    fontsize=10,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# True map at inference resolution\n",
    "map_inf.amp = amp_tru\n",
    "map_inf[1:, :] = y_tru[1 : (ydeg_inf + 1) ** 2]\n",
    "map_inf.show(\n",
    "    ax=ax[1], projection=\"moll\", colorbar=True, norm=cnorm(), cmap=cmap,\n",
    ")\n",
    "ax[1].annotate(\n",
    "    \"true (l={})\".format(ydeg_inf),\n",
    "    xy=(0, np.sqrt(2)),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(0, 5),\n",
    "    textcoords=\"offset points\",\n",
    "    ha=\"center\",\n",
    "    va=\"bottom\",\n",
    "    fontsize=10,\n",
    "    fontweight=\"bold\",\n",
    "    clip_on=False,\n",
    ")\n",
    "\n",
    "# Inferred maps\n",
    "for i in range(nmeth):\n",
    "    map_inf.amp = amp[i]\n",
    "    map_inf[1:, :] = y[i][1:]\n",
    "    map_inf.show(\n",
    "        ax=ax[i + 2], projection=\"moll\", colorbar=True, norm=cnorm(), cmap=cmap,\n",
    "    )\n",
    "    ax[i + 2].annotate(\n",
    "        labels[i],\n",
    "        xy=(0, np.sqrt(2)),\n",
    "        xycoords=\"data\",\n",
    "        xytext=(0, 5),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "        clip_on=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
