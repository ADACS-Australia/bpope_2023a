{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eclipsing binary: `pymc3` solution for the maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're going to do MCMC to infer the surface maps of two stars in an eclipsing binary given the light curve of the system. We generated the data in [this notebook](EclipsingBinary_Generate.ipynb). Note that here we assume we know everything else about the system (the orbital parameters, the limb darkening coefficients, etc.), so the only unknown parameters are the maps of the two stars, which are expressed in `starry` as vectors of spherical harmonic coefficients. In a future tutorial we'll explore a more complex inference problem where we have uncertainties on all the parameters.\n",
    "\n",
    "Let's begin with some imports. **Note that in order to do inference with pymc3, we need to enable lazy evaluation.** That's because `pymc3` requires derivatives of the likelihood function, so we need to use the fancy `theano` computational graph to perform backpropagation on the `starry` model. All this means in practice is that we'll have to call `.eval()` in some places to get numerical values out of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "%run notebook_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import pymc3_ext as pmx\n",
    "import exoplanet as xo\n",
    "import os\n",
    "import starry\n",
    "from corner import corner\n",
    "\n",
    "np.random.seed(12)\n",
    "starry.config.lazy = True\n",
    "starry.config.quiet = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Let's load the EB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input",
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Run the Generate notebook if needed\n",
    "if not os.path.exists(\"eb.npz\"):\n",
    "    import nbformat\n",
    "    from nbconvert.preprocessors import ExecutePreprocessor\n",
    "\n",
    "    with open(\"EclipsingBinary_Generate.ipynb\") as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    ep = ExecutePreprocessor(timeout=600, kernel_name=\"python3\")\n",
    "    ep.preprocess(nb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"eb.npz\", allow_pickle=True)\n",
    "A = data[\"A\"].item()\n",
    "B = data[\"B\"].item()\n",
    "t = data[\"t\"]\n",
    "flux = data[\"flux\"]\n",
    "sigma = data[\"sigma\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the primary, secondary, and system objects. Recall that we assume we know the true values of all the orbital parameters and star properties, *except* for the two surface maps. Note that we are instantiating the `starry` objects within a `pm.Model()` context so that `pymc3` can keep track of all the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "\n",
    "    # Primary\n",
    "    pri = starry.Primary(\n",
    "        starry.Map(ydeg=A[\"ydeg\"], udeg=A[\"udeg\"], inc=A[\"inc\"]),\n",
    "        r=A[\"r\"],\n",
    "        m=A[\"m\"],\n",
    "        prot=A[\"prot\"],\n",
    "    )\n",
    "    pri.map[1:] = A[\"u\"]\n",
    "\n",
    "    # Secondary\n",
    "    sec = starry.Secondary(\n",
    "        starry.Map(ydeg=B[\"ydeg\"], udeg=B[\"udeg\"], inc=B[\"inc\"]),\n",
    "        r=B[\"r\"],\n",
    "        m=B[\"m\"],\n",
    "        porb=B[\"porb\"],\n",
    "        prot=B[\"prot\"],\n",
    "        t0=B[\"t0\"],\n",
    "        inc=B[\"inc\"],\n",
    "    )\n",
    "    sec.map[1:] = B[\"u\"]\n",
    "\n",
    "    # System\n",
    "    sys = starry.System(pri, sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the light curve we're going to do inference on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 5))\n",
    "ax.plot(t, flux, \"k.\", alpha=0.5, ms=4)\n",
    "ax.set_xlabel(\"time [days]\", fontsize=24)\n",
    "ax.set_ylabel(\"normalized flux\", fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the `pymc3` model\n",
    "\n",
    "Now we define the full `pymc3` model. If you've never used `pymc3` before, Dan Foreman-Mackey's [exoplanet package documentation](https://exoplanet.dfm.io/en/stable/) has lots of nice tutorials on how to use `pymc3` to do inference. The basic idea here is we define our variables by assigning priors to them; we use a `pm.MvNormal` for both the primary and secondary maps. This is a multi-variate normal (Gaussian) distribution, which happens to be a convenient prior to place on spherical harmonic coefficients because of its close relationship to the *power spectrum* of the map. In particular, if the Gaussian prior is zero-mean and its covariance is diagonal with constant entries for each degree $l$ (as we assume below), this is equivalent to an isotropic prior whose power spectrum is given by those entries on the diagonal. Note that for simplicity we are assuming a *flat* power spectrum, meaning we place the same prior weight on all spatial scales. So the covariance of our Gaussian is as simple as it can be: it's just $\\lambda I$, where $\\lambda = 10^{-2}$ is the prior variance of the spherical harmonic coefficients and $I$ is the identity matrix. The scalar $\\lambda$ is essentially a regularization parameter: by making it small, we ensure that the spherical harmonic coefficients stay close to zero, which is usually what we want for physical maps.\n",
    "\n",
    "You'll note there's also a call to `pm.Deterministic`, which just keeps track of variables for later (in this case, we'll have access to the value of `flux_model` for every iteration of the chain once we're done; this is useful for plotting). And finally, there's a call to `pm.Normal` in which we specify our `observed` values, their standard deviation `sd`, and the mean vector `mu`, which is our `starry` flux model. This normal distribution is our chi-squared term: we're telling `pymc3` that our data is normally distributed about our model with some (observational) uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "\n",
    "    # The amplitude of the primary\n",
    "    pri.map.amp = pm.Normal(\"pri_amp\", mu=1.0, sd=0.1)\n",
    "\n",
    "    # The Ylm coefficients of the primary\n",
    "    # with a zero-mean isotropic Gaussian prior\n",
    "    ncoeff = pri.map.Ny - 1\n",
    "    pri_mu = np.zeros(ncoeff)\n",
    "    pri_cov = 1e-2 * np.eye(ncoeff)\n",
    "    pri.map[1:, :] = pm.MvNormal(\"pri_y\", pri_mu, pri_cov, shape=(ncoeff,))\n",
    "\n",
    "    # The amplitude of the secondary\n",
    "    sec.map.amp = pm.Normal(\"sec_amp\", mu=0.1, sd=0.01)\n",
    "\n",
    "    # The Ylm coefficients of the secondary\n",
    "    # with a zero-mean isotropic Gaussian prior\n",
    "    ncoeff = sec.map.Ny - 1\n",
    "    sec_mu = np.zeros(ncoeff)\n",
    "    sec_cov = 1e-2 * np.eye(ncoeff)\n",
    "    sec.map[1:, :] = pm.MvNormal(\"sec_y\", sec_mu, sec_cov, shape=(ncoeff,))\n",
    "\n",
    "    # Compute the flux\n",
    "    flux_model = sys.flux(t=t)\n",
    "\n",
    "    # Track some values for plotting later\n",
    "    pm.Deterministic(\"flux_model\", flux_model)\n",
    "\n",
    "    # Save our initial guess\n",
    "    flux_model_guess = pmx.eval_in_model(flux_model)\n",
    "\n",
    "    # The likelihood function assuming known Gaussian uncertainty\n",
    "    pm.Normal(\"obs\", mu=flux_model, sd=sigma, observed=flux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've specified the model, it's a good idea to run a quick gradient descent to find the MAP (maximum a posteriori) solution. This will give us a decent starting point for the inference problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    map_soln = pmx.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the dramatic increase in the value of the log posterior!\n",
    "Let's plot the MAP model alongside the data and the initial guess (note that we're doing quite well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(t, flux, \"k.\", alpha=0.3, ms=2, label=\"data\")\n",
    "plt.plot(t, flux_model_guess, \"C1--\", lw=1, alpha=0.5, label=\"Initial\")\n",
    "plt.plot(\n",
    "    t, pmx.eval_in_model(flux_model, map_soln, model=model), \"C1-\", label=\"MAP\", lw=1\n",
    ")\n",
    "plt.legend(fontsize=10, numpoints=5)\n",
    "plt.xlabel(\"time [days]\", fontsize=24)\n",
    "plt.ylabel(\"relative flux\", fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the corresponding maps: note that we recover the spots *really well*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = starry.Map(ydeg=A[\"ydeg\"])\n",
    "map.inc = A[\"inc\"]\n",
    "map.amp = map_soln[\"pri_amp\"]\n",
    "map[1:, :] = map_soln[\"pri_y\"]\n",
    "map.show(theta=np.linspace(0, 360, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = starry.Map(ydeg=B[\"ydeg\"])\n",
    "map.inc = B[\"inc\"]\n",
    "map.amp = map_soln[\"sec_amp\"]\n",
    "map[1:, :] = map_soln[\"sec_y\"]\n",
    "map.show(theta=np.linspace(0, 360, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC sampling\n",
    "\n",
    "We have an optimum solution, but we're really interested in the *posterior* over surface maps (i.e., an understanding of the uncertainty of our solution). We're therefore going to do MCMC sampling with `pymc3`. This is easy: within the `model` context, we just call `pmx.sample`. The number of tuning and draw steps below are quite small since I wanted this notebook to run quickly; try increasing them by a factor of a few to get more faithful posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pmx.sample(tune=500, draws=500, start=map_soln, chains=4, target_accept=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at `pm.summary` to check if things converged. In particular, we're looking for a large number of effective samples `ess` for all parameters and a value of `r_hat` that is very close to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = [\"pri_amp\", \"pri_y\", \"sec_amp\", \"sec_y\"]\n",
    "display(pm.summary(trace, var_names=varnames).head())\n",
    "display(pm.summary(trace, var_names=varnames).tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of effective samples for some of the parameters is quite small, so in practice we should run this chain for longer (an exercise for the reader!) But let's carry on for now, keeping in mind that our posteriors will be quite noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model for 24 random samples from the chain. Note that the lines are so close together that they're indistinguishable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(t, flux, \"k.\", alpha=0.3, ms=2, label=\"data\")\n",
    "label = \"samples\"\n",
    "for i in np.random.choice(range(len(trace[\"flux_model\"])), 24):\n",
    "    plt.plot(t, trace[\"flux_model\"][i], \"C0-\", alpha=0.3, label=label)\n",
    "    label = None\n",
    "plt.legend(fontsize=10, numpoints=5)\n",
    "plt.xlabel(\"time [days]\", fontsize=24)\n",
    "plt.ylabel(\"relative flux\", fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the *mean* map and a *random* sample to the true map for each star:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample\n",
    "np.random.seed(0)\n",
    "i = np.random.randint(len(trace[\"pri_y\"]))\n",
    "\n",
    "map = starry.Map(ydeg=A[\"ydeg\"])\n",
    "map[1:, :] = np.mean(trace[\"pri_y\"], axis=0)\n",
    "map.amp = np.mean(trace[\"pri_amp\"])\n",
    "pri_mu = map.render(projection=\"rect\").eval()\n",
    "map[1:, :] = trace[\"pri_y\"][i]\n",
    "map.amp = trace[\"pri_amp\"][i]\n",
    "pri_draw = map.render(projection=\"rect\").eval()\n",
    "map[1:, :] = A[\"y\"]\n",
    "map.amp = A[\"amp\"]\n",
    "pri_true = map.render(projection=\"rect\").eval()\n",
    "\n",
    "map = starry.Map(ydeg=B[\"ydeg\"])\n",
    "map[1:, :] = np.mean(trace[\"sec_y\"], axis=0)\n",
    "map.amp = np.mean(trace[\"sec_amp\"])\n",
    "sec_mu = map.render(projection=\"rect\").eval()\n",
    "map[1:, :] = trace[\"sec_y\"][i]\n",
    "map.amp = trace[\"sec_amp\"][i]\n",
    "sec_draw = map.render(projection=\"rect\").eval()\n",
    "map[1:, :] = B[\"y\"]\n",
    "map.amp = B[\"amp\"]\n",
    "sec_true = map.render(projection=\"rect\").eval()\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(8, 7))\n",
    "ax[0, 0].imshow(\n",
    "    pri_true,\n",
    "    origin=\"lower\",\n",
    "    extent=(-180, 180, -90, 90),\n",
    "    cmap=\"plasma\",\n",
    "    vmin=0,\n",
    "    vmax=0.4,\n",
    ")\n",
    "ax[1, 0].imshow(\n",
    "    pri_mu,\n",
    "    origin=\"lower\",\n",
    "    extent=(-180, 180, -90, 90),\n",
    "    cmap=\"plasma\",\n",
    "    vmin=0,\n",
    "    vmax=0.4,\n",
    ")\n",
    "ax[2, 0].imshow(\n",
    "    pri_draw,\n",
    "    origin=\"lower\",\n",
    "    extent=(-180, 180, -90, 90),\n",
    "    cmap=\"plasma\",\n",
    "    vmin=0,\n",
    "    vmax=0.4,\n",
    ")\n",
    "ax[0, 1].imshow(\n",
    "    sec_true,\n",
    "    origin=\"lower\",\n",
    "    extent=(-180, 180, -90, 90),\n",
    "    cmap=\"plasma\",\n",
    "    vmin=0,\n",
    "    vmax=0.04,\n",
    ")\n",
    "ax[1, 1].imshow(\n",
    "    sec_mu,\n",
    "    origin=\"lower\",\n",
    "    extent=(-180, 180, -90, 90),\n",
    "    cmap=\"plasma\",\n",
    "    vmin=0,\n",
    "    vmax=0.04,\n",
    ")\n",
    "ax[2, 1].imshow(\n",
    "    sec_draw,\n",
    "    origin=\"lower\",\n",
    "    extent=(-180, 180, -90, 90),\n",
    "    cmap=\"plasma\",\n",
    "    vmin=0,\n",
    "    vmax=0.04,\n",
    ")\n",
    "ax[0, 0].set_title(\"primary\")\n",
    "ax[0, 1].set_title(\"secondary\")\n",
    "ax[0, 0].set_ylabel(\"true\", rotation=0, labelpad=20)\n",
    "ax[1, 0].set_ylabel(\"mean\", rotation=0, labelpad=20)\n",
    "ax[2, 0].set_ylabel(\"draw\", rotation=0, labelpad=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good! There are obvious artifacts (there are tons of degeneracies in this problem), but we've definitely recovered the spots, with some uncertainty. Recall that our chains weren't well converged! Run this notebook for longer to get more faithful posteriors.\n",
    "\n",
    "Finally, here's a corner plot for the first several coefficients of the primary map. You can see that all the posteriors are nice and Gaussian, with some fairly strong correlations (the degeneracies I mentioned above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(9, 9, figsize=(7, 7))\n",
    "labels = [r\"$\\alpha$\"] + [\n",
    "    r\"$Y_{%d,%d}$\" % (l, m)\n",
    "    for l in range(1, pri.map.ydeg + 1)\n",
    "    for m in range(-l, l + 1)\n",
    "]\n",
    "samps = np.hstack((trace[\"pri_amp\"].reshape(-1, 1), trace[\"pri_y\"][:, :8]))\n",
    "corner(samps, fig=fig, labels=labels)\n",
    "for axis in ax.flatten():\n",
    "    axis.xaxis.set_tick_params(labelsize=6)\n",
    "    axis.yaxis.set_tick_params(labelsize=6)\n",
    "    axis.xaxis.label.set_size(12)\n",
    "    axis.yaxis.label.set_size(12)\n",
    "    axis.xaxis.set_label_coords(0.5, -0.6)\n",
    "    axis.yaxis.set_label_coords(-0.6, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! While sampling with `pymc3` is fairly fast, the problem of inferring a surface map when all other parameters are known is a **linear problem**, which means it actually has an *analytic* solution! In the following [notebook](EclipsingBinary_Linear.ipynb), we show how to take advantage of this within `starry` to do extremely fast inference."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
